# Fall 2023 Course Project Virginia Tech

Project Name: Is this for real? Hallucination analysis in LLMs

Course Name: Security Risks in Generative AI

Project Members: Sindhuja Madabushi, Zhegjie Ji 

Description:
In this project, we study the phenomenon of hallucination in large language models (LLMs), where the models generate inaccurate or fabricated information. To understand and detect these hallucinations, we present a methodology for creating a dataset that triggers LLMs to produce hallucination responses, specifically within the framework of context-based question-answering tasks.

Objectives
Primary Objective: Generate a QA dataset for hallucination
Secondary Objectives: perturb LLM decoding strategies such as temperature, presence penalty, frequency penalty, and top-p sampling, and observe their influence on hallucination rates 

Prerequisites: Python3, OpenAI libraries installed

Contact Sindhuja Madabushi at msindhuja@vt.edu for questions.
